{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech recognition\n",
    "for https://www.kaggle.com/c/pass-cmu-dl-together-homework-1-part-2-spring-19/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader - given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class WSJ():\n",
    "    \"\"\" Load the WSJ speech dataset\n",
    "        \n",
    "        Ensure WSJ_PATH is path to directory containing \n",
    "        all data files (.npy) provided on Kaggle.\n",
    "        \n",
    "        Example usage:\n",
    "            loader = WSJ()\n",
    "            trainX, trainY = loader.train\n",
    "            assert(trainX.shape[0] == 24590)\n",
    "            \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self):\n",
    "        self.dev_set = None\n",
    "        self.train_set = None\n",
    "        self.test_set = None\n",
    "  \n",
    "    @property\n",
    "    def dev(self):\n",
    "        if self.dev_set is None:\n",
    "            self.dev_set = load_raw(os.environ['WSJ_PATH'], 'dev')\n",
    "        return self.dev_set\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        if self.train_set is None:\n",
    "            self.train_set = load_raw(os.environ['WSJ_PATH'], 'train')\n",
    "        return self.train_set\n",
    "  \n",
    "    @property\n",
    "    def test(self):\n",
    "        if self.test_set is None:\n",
    "            self.test_set = (np.load(os.path.join(os.environ['WSJ_PATH'], 'test.npy'), encoding='bytes'), None)\n",
    "        return self.test_set\n",
    "    \n",
    "def load_raw(path, name):\n",
    "    return (\n",
    "        np.load(os.path.join(path, '{}.npy'.format(name)), encoding='bytes'), \n",
    "        np.load(os.path.join(path, '{}_labels.npy'.format(name)), encoding='bytes')\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "Consider a frame in the contenxt\n",
    "Used to prepend L and append R frames after the current one. Pad frames where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    L, R : number of elements to add to left and right\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, L, R, pad_mode='constant', pad_constant=0):\n",
    "        self.L, self.R = (L, R)\n",
    "        self.y = None if y is None else np.concatenate(y) \n",
    "        #indexes of utterances\n",
    "        self.cumindex = np.cumsum([i.shape[0] for i in X])\n",
    "        \n",
    "        X_pad = []\n",
    "        \n",
    "        for utt in X:\n",
    "            utt_new = np.pad(utt, ((L, R), (0,0)), pad_mode) #, constant_values=pad_constant)\n",
    "            X_pad.append(utt_new)\n",
    "            \n",
    "        self.X = np.concatenate(X_pad)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.cumindex[-1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #old to new index\n",
    "        old_utt = np.searchsorted(self.cumindex-1, index)\n",
    "\n",
    "        new_index = index + self.L + old_utt * (self.L + self.R)\n",
    "        \n",
    "        x = self.X[new_index - self.L : new_index + self.R + 1].ravel()\n",
    "        \n",
    "        if self.y is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x, self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WSJ_PATH']=\"/media/data/class-cmu-dl/hw1part2/\"\n",
    "loader = WSJ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = loader.train\n",
    "assert(trainX.shape[0] == 24590)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1103,), (1103,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devX, devY = loader.dev\n",
    "devX.shape, devY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge dev and train, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25693,), (25693,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX = np.append(trainX, devX)\n",
    "trainY = np.append(trainY, devY)\n",
    "trainX.shape, trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "trainX, trainY = shuffle(trainX, trainY, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17985"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio=0.7\n",
    "split_index = round(trainX.shape[0] * split_ratio)\n",
    "split_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX = devX[:split_index]\n",
    "# trainY = devY[:split_index]\n",
    "\n",
    "# devX = devX[split_index:]\n",
    "# devY = devY[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX = trainX[split_index:]\n",
    "devY = trainY[split_index:]\n",
    "\n",
    "trainX = trainX[:split_index]\n",
    "trainY = trainY[:split_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helps to debug CUDA errors\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_gpu = False\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    train_on_gpu = True\n",
    "    device = 'cuda'\n",
    "train_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#padding\n",
    "Left = 7\n",
    "Right = 3\n",
    "\n",
    "train_dataset = ContextDataset(\n",
    "    trainX, # device=device), \n",
    "    trainY, #, device=device)\n",
    "    Left, Right, #L,R\n",
    "    pad_mode='edge',\n",
    ")\n",
    "\n",
    "val_dataset = ContextDataset(\n",
    "    devX,#, device=device), \n",
    "    devY,#, device=device)\n",
    "    Left, Right,\n",
    "    pad_mode='edge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          num_workers=4) \n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        #shuffle=True,\n",
    "                        #drop_last=True,\n",
    "                        num_workers=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_size = 40\n",
    "\n",
    "input_dim = frame_size * (Left + 1 + Right) # in the context\n",
    "hidden1_dim = 2048\n",
    "hidden2_dim = 512\n",
    "output_dim = 138\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, hidden1_dim, bias=False),\n",
    "    torch.nn.BatchNorm1d(hidden1_dim, momentum=0.1),\n",
    "    #orch.nn.ReLU(),\n",
    "    torch.nn.Sigmoid(),\n",
    "#     torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden1_dim, hidden2_dim, bias=False),\n",
    "    torch.nn.BatchNorm1d(hidden2_dim, momentum=0.1),\n",
    "    torch.nn.Sigmoid(),\n",
    "#     #torch.nn.ReLU(),\n",
    "#     torch.nn.LeakyReLU(),\n",
    "#     torch.nn.Linear(hidden1_dim, hidden2_dim, bias=False),\n",
    "#     torch.nn.BatchNorm1d(hidden2_dim, momentum=0.1),\n",
    "#     #torch.nn.ReLU(),\n",
    "#     torch.nn.LeakyReLU(),\n",
    "    \n",
    "    torch.nn.Linear(hidden2_dim, output_dim) #, bias=False),\n",
    "    #torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=440, out_features=2048, bias=False)\n",
       "  (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): Sigmoid()\n",
       "  (3): Linear(in_features=2048, out_features=512, bias=False)\n",
       "  (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Sigmoid()\n",
       "  (6): Linear(in_features=512, out_features=138, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight initializtion\n",
    "#torch.nn.init.xavier_uniform(model.weight)\n",
    "#https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight,  gain=nn.init.calculate_gain('relu'))\n",
    "        #only fill bias if enabled in the layer\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "model = model.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation helper\n",
    "def validate(model, val_loader):\n",
    "    losses = []\n",
    "    corrects = 0\n",
    "    n_labels = 0\n",
    "    \n",
    "    for data, labels in val_loader:\n",
    "        if train_on_gpu:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        _, pred = torch.max(output, 1)\n",
    "        correct = (pred == labels).float().sum()\n",
    "        corrects += correct\n",
    "        n_labels += len(labels)\n",
    "    \n",
    "        return sum(losses)/len(losses), corrects/n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 - b-loss: 1.746, Accuracy: 0.562\n",
      "       Valid Epoch 1 - loss: 1.552, Accuracy: 0.588\n",
      "Train Epoch 2 - b-loss: 1.613, Accuracy: 0.576\n",
      "       Valid Epoch 2 - loss: 1.465, Accuracy: 0.607\n",
      "Train Epoch 3 - b-loss: 1.597, Accuracy: 0.576\n",
      "       Valid Epoch 3 - loss: 1.420, Accuracy: 0.592\n",
      "Train Epoch 4 - b-loss: 1.786, Accuracy: 0.539\n",
      "       Valid Epoch 4 - loss: 1.426, Accuracy: 0.600\n",
      "Train Epoch 5 - b-loss: 1.576, Accuracy: 0.594\n",
      "       Valid Epoch 5 - loss: 1.394, Accuracy: 0.617\n",
      "Train Epoch 6 - b-loss: 1.744, Accuracy: 0.529\n",
      "       Valid Epoch 6 - loss: 1.372, Accuracy: 0.623\n",
      "Train Epoch 7 - b-loss: 1.669, Accuracy: 0.578\n",
      "       Valid Epoch 7 - loss: 1.397, Accuracy: 0.633\n",
      "Train Epoch 8 - b-loss: 1.505, Accuracy: 0.602\n",
      "       Valid Epoch 8 - loss: 1.349, Accuracy: 0.621\n",
      "Train Epoch 9 - b-loss: 1.492, Accuracy: 0.588\n",
      "       Valid Epoch 9 - loss: 1.367, Accuracy: 0.613\n",
      "Train Epoch 10 - b-loss: 1.643, Accuracy: 0.596\n",
      "       Valid Epoch 10 - loss: 1.360, Accuracy: 0.619\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    correct = (predicted == labels).float().sum()\n",
    " \n",
    "    print(\"Train Epoch {} - b-loss: {:.3f}, Accuracy: {:.3f}\".format(n_epochs_total+epoch+1,\n",
    "                                                               loss.cpu().item(), \n",
    "                                                               correct/output.shape[0]))\n",
    "    #accuracy on val\n",
    "    model.eval()\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    print(\"       Valid Epoch {} - loss: {:.3f}, Accuracy: {:.3f}\".format(n_epochs_total+epoch+1,\n",
    "                                                               val_loss.cpu().item(), \n",
    "                                                               val_acc.cpu().item()))\n",
    "n_epochs_total += n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop right here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on full dataset (if previously trained on dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = loader.train\n",
    "assert(trainX.shape[0] == 24590)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ContextDataset(\n",
    "    trainX, # device=device), \n",
    "    trainY, #, device=device)\n",
    "    Left, Right #L,R\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                          drop_last=True,                          \n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = 0.005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #accuracy on train (last batch)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    correct = (predicted == labels).float().sum()\n",
    " \n",
    "    print(\"Train Epoch {} - b-loss: {:.3f}, Accuracy: {:.3f}\".format(epoch+1,\n",
    "                                                               loss.cpu().item(), \n",
    "                                                               correct/output.shape[0]))\n",
    "    #accuracy on val\n",
    "    model.eval()\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    print(\"     Valid Epoch {} - loss: {:.3f}, Accuracy: {:.3f}\".format(epoch+1,\n",
    "                                                               val_loss.cpu().item(), \n",
    "                                                               val_acc.cpu().item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY = loader.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ContextDataset(\n",
    "    testX,#, device=device), \n",
    "    None,\n",
    "    Left, Right\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = []\n",
    "model.eval()\n",
    "for data in test_loader:\n",
    "        if train_on_gpu:\n",
    "            data = data.to(device)\n",
    "        output = model(data)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        preds.extend(pred.cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sub = pd.read_csv(\"sample submission.csv\", index_col=0)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['label']=preds\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sub10.cvs\", \"w\") as subf:\n",
    "    subf.write(sub.to_csv())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
